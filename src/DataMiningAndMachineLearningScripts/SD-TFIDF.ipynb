{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##########################################################################\n",
    "# Author: Christopher Thomas Goodwin\n",
    "# Creation Date: 2024.05.16\n",
    "# Summary: Uses sklearn to explore the data of the NSHWE Stimmungs- und Lageberichte files\n",
    "#          using TF-IDF and then K-Means clustering\n",
    "##########################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:45:47.771117Z",
     "start_time": "2024-05-17T03:45:47.762068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### Get all file names\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "files = [] # holds file paths of all text files\n",
    "\n",
    "for file in Path(\"../../data/text\").glob(\"*.txt\"):\n",
    "    files.append(file.parent / file.name)\n",
    "\n",
    "n_files = len(files)\n",
    "print(n_files) # 750\n",
    "print(files[0]) # should have full path"
   ],
   "id": "feb5aa81c8755079",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "../../data/text/1943.02.04 - Meldungen aus dem Reich Cleaned.txt\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:45:52.800470Z",
     "start_time": "2024-05-17T03:45:52.646504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### convert all text files into strings\n",
    "docs = [] # holds string version of each file (i.e. each text file converted into one string and stored here\n",
    "\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        contents = f.read()\n",
    "    docs.append(contents)\n",
    "    f.close()"
   ],
   "id": "1857f53849b2d6c9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:46:10.979752Z",
     "start_time": "2024-05-17T03:46:05.546686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### run TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "additional_stop_words = [\"volk\", \"volksgemeinschaft\", \"1939\", \"1940\", \"1941\", \"1942\", \"1943\", \"1944\", \"1945\", \"deutsch\", \"bevÃ¶lkerung\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"einzelmeldungen\", \"volksgenossen\", \"sei\", \"seien\", \"worden\", \"meldungen\", \"deutsche\", \"deutschen\", \"wegen\", \"wurde\", \"gif\", \"pro\", \"kg\", \"minusbox\", \"images\", \"rm\"]\n",
    "\n",
    "for i in range(0, 1946):\n",
    "    additional_stop_words.append(str(i))\n",
    "\n",
    "german_stop_words.extend(additional_stop_words)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=.90, min_df=5, stop_words=german_stop_words, use_idf=True, norm=None, lowercase=True, ngram_range=(1,3))\n",
    "transformed_documents = vectorizer.fit_transform(docs)\n",
    "transformed_documents_array = transformed_documents.toarray()\n",
    "print(len(transformed_documents_array)) # should match the number above to make sure we transformed all documents"
   ],
   "id": "94cae13bca1b5a5f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cgoodwin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:46:53.864369Z",
     "start_time": "2024-05-17T03:46:16.755687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### output TF-IDF scores\n",
    "import pandas as pd\n",
    "\n",
    "Path(\"data/tf-idf_output\").mkdir(parents=True, exist_ok=True) # make directory if it doesn't already exist\n",
    "\n",
    "output_filenames = [str(file).replace(\".txt\", \".csv\").replace(\"../../data/text\", \"data/tf-idf_output\") for file in files]\n",
    "\n",
    "for counter, doc in enumerate(transformed_documents_array):\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    one_doc_as_df.to_csv(output_filenames[counter])"
   ],
   "id": "c438354b70bbfb7b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:49:48.479282Z",
     "start_time": "2024-05-17T03:49:45.322745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### Get keywords\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = transformed_documents.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "all_keywords = []\n",
    "\n",
    "for doc in denselist:\n",
    "    x = 0\n",
    "    keywords = []\n",
    "    for word in doc:\n",
    "        if word > 0:\n",
    "            keywords.append(feature_names[x])\n",
    "        x += 1\n",
    "    all_keywords.append(keywords)"
   ],
   "id": "e7e77260b634e45b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:56:56.298781Z",
     "start_time": "2024-05-17T03:56:54.983077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### K-Means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 75\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "kmeans_model.fit(transformed_documents)\n",
    "\n",
    "order_centroids = kmeans_model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "with open (\"data/kmeans_output/kmeans_results.txt\", 'w') as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i}\\n\")\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            f.write(f\"{terms[ind]}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ],
   "id": "6567333e6cb31297",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
