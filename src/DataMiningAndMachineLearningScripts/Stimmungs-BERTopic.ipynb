{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9eebbea6367738c",
   "metadata": {},
   "source": [
    "# BERTopic Exploration of the Stimmmungs- und Lageberichte Database Files\n",
    "**Author:** Christopher Thomas Goodwin\n",
    "\n",
    "**Creation Date:** 2024.04.10\n",
    "\n",
    "**Summary:** Uses BERTopic modelling to explore the data of the NSHWE Stimmungs- und Lageberichte files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32d9a89f70746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "from bertopic import BERTopic\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "if len(GPUtil.getAvailable()) > 0:\n",
    "    from cuml.cluster import HDBSCAN\n",
    "    from cuml.manifold import UMAP\n",
    "else:\n",
    "    from umap import UMAP\n",
    "    from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b681f81e40e3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    path = \"/home/cgoodwin/PycharmProjects/NSHWEDatabaseMining/data/json/stimmungs_data_sentences.json\"\n",
    "elif platform == \"darwin\":\n",
    "    path = \"/Users/cgoodwin/Documents/Programming/Python/NSHWEDatabaseMining/data/json/stimmungs_data_sentences.json\"\n",
    "else:\n",
    "    path = \"C:\\Users\\Christopher Goodwin\\Documents\\Programming Projects\\TextMiningNaziIdeology\\data\\json\\stimmungs_data_sentences.json\"\n",
    "    \n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    files = json.load(f)\n",
    "    # files loaded in as dictionary with strings of 0... length of files\n",
    "    \n",
    "    # we want just the textual data, the report from each entry\n",
    "    reports = []\n",
    "    for i in range(len(files)):\n",
    "        reports.append(files[str(i)][\"report\"]) # iterate through dictionary and append report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0c2b8c2936ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vectorizer for German stopwords\n",
    "german_stop_words = stopwords.words('german')\n",
    "additional_stop_words = [\"volk\", \"volksgemeinschaft\", \"1939\", \"1940\", \"1941\", \"1942\", \"1943\", \"1944\", \"1945\", \"deutsch\", \"bevÃ¶lkerung\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"einzelmeldungen\", \"volksgenossen\", \"sei\", \"seien\", \"worden\", \"meldungen\", \"deutsche\", \"deutschen\", \"wegen\", \"wurde\", \"gif\", \"pro\", \"kg\", \"minusbox\", \"images\", \"rm\"]\n",
    "\n",
    "for i in range(0, 1946):\n",
    "    additional_stop_words.append(str(i))\n",
    "\n",
    "german_stop_words.extend(additional_stop_words)\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=german_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517a6c0066c782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for GPU to run model faster\n",
    "if len(GPUtil.getAvailable()) > 0:\n",
    "    print(\"GPU engaged.\")\n",
    "    \n",
    "    # create instances of GPU-accelerated UMAP and HDBSCAN\n",
    "    umap_model = UMAP(n_components=2, n_neighbors=15, min_dist=0.0)\n",
    "    hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True, prediction_data=True)\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", vectorizer_model=vectorizer_model, language=\"multilingual\", nr_topics=75, top_n_words=10, calculate_probabilities=True, verbose=True)\n",
    "else:\n",
    "    print(\"No GPU engaged.\")\n",
    "    \n",
    "    umap_model = UMAP(n_components=2, n_neighbors=15, min_dist=0.0)\n",
    "    hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True, prediction_data=True)\n",
    "    \n",
    "    # use multilingual model and apply German stopwords vectorizer model\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", vectorizer_model=vectorizer_model, language=\"multilingual\", nr_topics=75, top_n_words=10, calculate_probabilities=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167cd9e1ea07ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968646d520c3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491dccec5f0d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e224f6c7e6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680519dca268321",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\"my_model\", serialization=\".safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4e12cf6bc8617",
   "metadata": {},
   "source": [
    "# Apply TF-IDF to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d6dc5e6463a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=5, stop_words=german_stop_words)\n",
    "embeddings = tfidf_vectorizer.fit_transform(reports)\n",
    "\n",
    "tfidf_model = BERTopic(nr_topics=75)\n",
    "tfidf_topics, tfidf_probs = tfidf_model.fit(reports, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b492a8aa208e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model.get_topic_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
